{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46a8a7a0",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "\n",
    "<h2>Homework - Hyperparameter Tuning</h2>\n",
    "<h4>DAT-5303 | Machine Learning</h4>\n",
    "\n",
    "<br>\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894d6d3d",
   "metadata": {},
   "source": [
    "**Team 14 :** \n",
    "- Zaynab Zennour\n",
    "- Sreekar Bathula\n",
    "- Olubiyi George\n",
    "- Deena Jivan\n",
    "- Akoh Jackson Udeng\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb8e6f",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<br>\n",
    "The models we will use : \n",
    "\n",
    "* Random Forest\n",
    "* Gradient Boosted Machine (GBM)\n",
    "\n",
    "<h3>Step 1: Imports</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7173da84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing critical libraries\n",
    "import pandas            as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn           as sns\n",
    "import itertools\n",
    "import numpy as np \n",
    "\n",
    "\n",
    "# importing model types\n",
    "import sklearn.linear_model                            # linear models \n",
    "from sklearn.tree     import DecisionTreeRegressor     # regression trees\n",
    "from sklearn.ensemble import RandomForestRegressor     # random forest \n",
    "from sklearn.ensemble import GradientBoostingRegressor # GBM\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "\n",
    "\n",
    "# importing ML tools\n",
    "from sklearn.model_selection import train_test_split   \n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "# loading data and setting display options\n",
    "housing = pd.read_excel('./__datasets/housing_feature_rich.xlsx')\n",
    "\n",
    "\n",
    "# setting pandas print options\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22694f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_variables = ['Garage_Cars', 'Overall_Qual', 'Total_Bsmt_SF',\n",
    "               'NridgHt', 'Kitchen_AbvGr', 'has_Second_Flr',\n",
    "               'Mas_Vnr_Area', 'has_Garage', 'Porch_Area',\n",
    "               'NWAmes', 'OldTown', 'Overall_Cond',\n",
    "               'Edwards', 'Somerst', 'Fireplaces',\n",
    "               'Second_Flr_SF', 'First_Flr_SF', 'has_Mas_Vnr',\n",
    "               'CulDSac', 'Total_Bath', 'Crawfor', 'Garage_Area',\n",
    "               'has_Porch']\n",
    "\n",
    "\n",
    "full_x = ['Overall_Qual', 'Overall_Cond', 'Mas_Vnr_Area', 'Total_Bsmt_SF',\n",
    "          'First_Flr_SF', 'Second_Flr_SF', 'Gr_Liv_Area', 'Full_Bath',\n",
    "          'Half_Bath', 'Kitchen_AbvGr', 'TotRms_AbvGr', 'Fireplaces',\n",
    "          'Garage_Cars', 'Garage_Area', 'Porch_Area', 'log_Lot_Area',\n",
    "          'has_Second_Flr', 'has_Garage', 'has_Mas_Vnr', 'has_Porch',\n",
    "          'Total_Bath', 'CulDSac', 'BrkSide', 'CollgCr', 'Crawfor',\n",
    "          'Edwards', 'Gilbert', 'Mitchel', 'NWAmes', 'NridgHt', 'OldTown',\n",
    "          'Sawyer', 'SawyerW', 'Somerst', 'Other_NH']\n",
    "\n",
    "\n",
    "reduced_x = ['Overall_Qual', 'Gr_Liv_Area', 'Full_Bath',\n",
    "             'Kitchen_AbvGr', 'TotRms_AbvGr', 'Fireplaces',\n",
    "             'Garage_Cars', 'Garage_Area', 'Porch_Area', \n",
    "             'log_Lot_Area', 'has_Second_Flr', 'has_Garage',\n",
    "             'has_Mas_Vnr', 'has_Porch', 'Total_Bath', 'CulDSac']\n",
    "\n",
    "housing['log_Sale_Price'] = np.log(housing['Sale_Price'])\n",
    "\n",
    "########################################\n",
    "# checking results\n",
    "########################################\n",
    "housing.head(n = 5)\n",
    "# preparing x-variables\n",
    "# x_data = housing.loc[ : , reduced_x ] # x-variables (can change this)\n",
    "x_data = housing.drop(['Sale_Price', 'log_Sale_Price'], axis = 1)\n",
    "\n",
    "\n",
    "# preparing y-variable\n",
    "#y_data = housing.loc[ : , 'Sale_Price']    # y-variable\n",
    "y_data = housing.loc[ : , 'log_Sale_Price'] # y-variable\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0099269b",
   "metadata": {
    "deletable": false,
    "editable": false,
    "run_control": {
     "frozen": true
    }
   },
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c77cbf",
   "metadata": {},
   "source": [
    "The models we will use for tuning are :\n",
    "- Random Forest\n",
    "- Gradient Boosted Machine (GBM)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9daaf31f",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<h3>Step 2: Train-Test Split</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4bf81991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split with stratification\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "            x_data,\n",
    "            y_data,\n",
    "            test_size    = 0.25,\n",
    "            random_state = 219 )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f095cc33",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<h3>Step 3: Checking Available Hyperparameters</h3>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d382bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling help() on each of our models : GradientBoostingRegressor\n",
    "help(GradientBoostingRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da6375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calling help() on each of our models : RandomForestRegressor\n",
    "help(RandomForestRegressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f9ef5",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<h3>Step 4: Model Development - Default Hyperparameters</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16d3f12",
   "metadata": {},
   "source": [
    "- **Random Forest Regressor** : Using default Hyperparameters and results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c39479ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    Model  n_estimators  max_depth  min_samples_leaf  Train R-Square  Test R-Square\n",
      "Random Forest (default 1)           100          2                 5           0.641          0.640\n",
      "Random Forest (default 2)           200          3                 4           0.765          0.753\n",
      "Random Forest (default 3)           300          5                 3           0.867          0.831\n"
     ]
    }
   ],
   "source": [
    "# Define the default hyperparameters for Random Forest (1)\n",
    "rf_default_params_1 = {'n_estimators': 100, 'max_depth': 2, 'min_samples_leaf': 5}\n",
    "rf_default_1 = RandomForestRegressor(**rf_default_params_1)\n",
    "rf_default_1.fit(X_train, y_train)\n",
    "rf_default_train_score_1 = r2_score(y_train, rf_default_1.predict(X_train)).round(3)\n",
    "rf_default_test_score_1 = r2_score(y_test, rf_default_1.predict(X_test)).round(3)\n",
    "\n",
    "# Define the default hyperparameters for Random Forest (2)\n",
    "rf_default_params_2 = {'n_estimators': 200, 'max_depth': 3,  'min_samples_leaf': 4}\n",
    "rf_default_2 = RandomForestRegressor(**rf_default_params_2)\n",
    "rf_default_2.fit(X_train, y_train)\n",
    "rf_default_train_score_2 = r2_score(y_train, rf_default_2.predict(X_train)).round(3)\n",
    "rf_default_test_score_2 = r2_score(y_test, rf_default_2.predict(X_test)).round(3)\n",
    "\n",
    "\n",
    "# Define the default hyperparameters for Random Forest (3)\n",
    "rf_default_params_3 = {'n_estimators': 300, 'max_depth': 5, 'min_samples_leaf': 3}\n",
    "rf_default_3 = RandomForestRegressor(**rf_default_params_3)\n",
    "rf_default_3.fit(X_train, y_train)\n",
    "rf_default_train_score_3 = r2_score(y_train, rf_default_3.predict(X_train)).round(3)\n",
    "rf_default_test_score_3 = r2_score(y_test, rf_default_3.predict(X_test)).round(3)\n",
    "\n",
    "# Define a dictionary to store the results\n",
    "results_dict = {\n",
    "    'Model': ['Random Forest (default 1)', 'Random Forest (default 2)', 'Random Forest (default 3)'],\n",
    "    'n_estimators': [rf_default_params_1['n_estimators'], rf_default_params_2['n_estimators'], rf_default_params_3['n_estimators']],\n",
    "    'max_depth': [rf_default_params_1['max_depth'], rf_default_params_2['max_depth'], rf_default_params_3['max_depth']],\n",
    "    'min_samples_leaf': [rf_default_params_1['min_samples_leaf'], rf_default_params_2['min_samples_leaf'], rf_default_params_3['min_samples_leaf']],\n",
    "    'Train R-Square': [rf_default_train_score_1, rf_default_train_score_2, rf_default_train_score_3],\n",
    "    'Test R-Square': [rf_default_test_score_1, rf_default_test_score_2, rf_default_test_score_3]\n",
    "}\n",
    "\n",
    "# Create a Pandas DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Print the results in tabular format\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591ef146",
   "metadata": {},
   "source": [
    "- **Gradient Boosting Regressor** : Using default Hyperparameters and results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccbb3912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Model  n_estimators  max_depth  min_samples_split  Train R-Square  Test R-Square\n",
      "Gradient Boosted Machine (default 1)           100          3                  4           0.931          0.883\n",
      "Gradient Boosted Machine (default 2)           200          4                  2           0.968          0.886\n",
      "Gradient Boosted Machine (default 3)           300          5                  6           0.990          0.882\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Default hyperparameters for Gradient Boosted Machine (GBM) (1)\n",
    "gbm_default_params_1 = {'n_estimators': 100, 'max_depth': 3, 'min_samples_split': 4}\n",
    "gbm_default_1 = GradientBoostingRegressor(**gbm_default_params_1)\n",
    "gbm_default_1.fit(X_train, y_train)\n",
    "gbm_default_train_score_1 = r2_score(y_train, gbm_default_1.predict(X_train)).round(3)\n",
    "gbm_default_test_score_1 = r2_score(y_test, gbm_default_1.predict(X_test)).round(3)\n",
    "\n",
    "# Default hyperparameters for Gradient Boosted Machine (GBM) (2)\n",
    "gbm_default_params_2 = {'n_estimators': 200, 'max_depth': 4, 'min_samples_split': 2}\n",
    "gbm_default_2 = GradientBoostingRegressor(**gbm_default_params_2)\n",
    "gbm_default_2.fit(X_train, y_train)\n",
    "gbm_default_train_score_2 = r2_score(y_train, gbm_default_2.predict(X_train)).round(3)\n",
    "gbm_default_test_score_2 = r2_score(y_test, gbm_default_2.predict(X_test)).round(3)\n",
    "\n",
    "# Default hyperparameters for Gradient Boosted Machine (GBM) (3)\n",
    "gbm_default_params_3 = {'n_estimators': 300, 'max_depth': 5, 'min_samples_split': 6}\n",
    "gbm_default_3 = GradientBoostingRegressor(**gbm_default_params_3)\n",
    "gbm_default_3.fit(X_train, y_train)\n",
    "gbm_default_train_score_3 = r2_score(y_train, gbm_default_3.predict(X_train)).round(3)\n",
    "gbm_default_test_score_3 = r2_score(y_test, gbm_default_3.predict(X_test)).round(3)\n",
    "\n",
    "\n",
    "# Define a dictionary to store the results\n",
    "results_dict = {\n",
    "    'Model': ['Gradient Boosted Machine (default 1)', 'Gradient Boosted Machine (default 2)', 'Gradient Boosted Machine (default 3)'],\n",
    "    'n_estimators': [gbm_default_params_1['n_estimators'], gbm_default_params_2['n_estimators'], gbm_default_params_3['n_estimators']],\n",
    "    'max_depth': [gbm_default_params_1['max_depth'], gbm_default_params_2['max_depth'], gbm_default_params_3['max_depth']],\n",
    "    'min_samples_split': [gbm_default_params_1['min_samples_split'], gbm_default_params_2['min_samples_split'], gbm_default_params_3['min_samples_split']],\n",
    "    'Train R-Square': [gbm_default_train_score_1, gbm_default_train_score_2, gbm_default_train_score_3],\n",
    "    'Test R-Square': [gbm_default_test_score_1, gbm_default_test_score_2, gbm_default_test_score_3]\n",
    "}\n",
    "\n",
    "# Create a Pandas DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Print the results in tabular format\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedf81be",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<h3>Step 5: Hyperparameter Tuning Part I</h3><br>\n",
    "<br><br>\n",
    "<strong>Requirement:</strong>  at least three hyperparameters per model.<br>\n",
    "<strong>Recommendation:</strong> Keep the number of models (i.e., iterations) you are building to:\n",
    "\n",
    "* Less than 2,000 for Random Forest and GBM  (not including cross-validation)\n",
    "* Less than 10,000 for all other model types (not including cross-validation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "16a131b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Random Forest models to be built: 1125\n"
     ]
    }
   ],
   "source": [
    "# Define ranges for each of the hyperparameters we would like tune. \n",
    "# this code calculates how many models we are building (based on the combinations of hyperparameters we are tuning).\n",
    "# parameters grid for RF (option 1) \n",
    "criterion_range = [\"squared_error\", \"absolute_error\", \"poisson\" ]\n",
    "split_range  = [ 1, 5, 10, 15, 20]\n",
    "depth_range     = range(5, 30, 10)\n",
    "leaf_range      = range(1, 50 ,2)   \n",
    "\n",
    "#define the product of possible combinations for each parameter \n",
    "params_grid = itertools.product( depth_range, split_range, leaf_range, criterion_range)\n",
    "\n",
    "num_models = len(list(params_grid))\n",
    "print(f\"Number of Random Forest models to be built: {num_models}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4b23cd57",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Random Forest models to be built: 1425\n"
     ]
    }
   ],
   "source": [
    "# parameters grid for RF (option 2) \n",
    "criterion_range = [\"squared_error\" ]\n",
    "split_range  = [ 1, 5, 10, 15, 20]\n",
    "depth_range     = range(5, 100, 5)\n",
    "leaf_range      = range(1, 30 ,2)\n",
    "\n",
    "params_grid = itertools.product( depth_range, split_range, leaf_range, criterion_range)\n",
    "\n",
    "num_models = len(list(params_grid))\n",
    "print(f\"Number of Random Forest models to be built: {num_models}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fbda6d97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Gradient Boosting Regressor models to be built: 1392\n"
     ]
    }
   ],
   "source": [
    "# Define ranges for each of the hyperparameters we would like tune. \n",
    "# this code calculates how many models we are building (based on the combinations of hyperparameters we are tuning).\n",
    "criterion_range = ['friedman_mse', 'squared_error', 'mse' ]\n",
    "split_range  = [2, 5, 10, 15]\n",
    "depth_range     = range(2, 6 ,1)  \n",
    "leaf_range      = range(2, 60 ,2)   \n",
    "\n",
    "#define the product of possible combinations for each parameter \n",
    "params_grid = itertools.product( depth_range, split_range, leaf_range, criterion_range)\n",
    "\n",
    "num_models = len(list(params_grid))\n",
    "print(f\"Number of Gradient Boosting Regressor models to be built: {num_models}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdfe6ae",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<h3>Step 6: Hyperparameter Tuning Part II</h3><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69c3cdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "criterion_range = [\"squared_error\", \"absolute_error\", \"poisson\" ]\n",
    "split_range  = [ 1, 5, 10, 15, 20]\n",
    "depth_range     = range(5, 30, 10)\n",
    "leaf_range      = range(1, 50 ,2)   \n",
    "\n",
    "\n",
    "\n",
    "# creating a hyperparameter grid for RF\n",
    "\n",
    "param_grid_rf = {'criterion' : criterion_range,\n",
    "                 'min_samples_split' : split_range,\n",
    "                 'max_depth' : depth_range,\n",
    "                 'min_samples_leaf' : leaf_range\n",
    "                 }\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "# hyperparameter grid for GBM \n",
    "\n",
    "criterion = ['friedman_mse', 'squared_error', 'mse' ]\n",
    "split  = [2, 5, 10, 15]\n",
    "depth     = range(2, 6 ,1)  \n",
    "leaf      = range(2, 60 ,2)   \n",
    "\n",
    "\n",
    "\n",
    "param_grid_gbm   = {'criterion' : criterion,\n",
    "                    'min_samples_split' : split,\n",
    "                    'max_depth' : depth,\n",
    "                    'min_samples_leaf' : leaf }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a687f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# declaring a hyperparameter space\n",
    "criterion_range = [\"squared_error\", \"absolute_error\", \"poisson\" ]\n",
    "split_range  = [10, 15]\n",
    "depth_range     = range(5, 30, 10)\n",
    "leaf_range      = range(1, 60 ,2) \n",
    "\n",
    "# creating a hyperparameter grid for RF\n",
    "\n",
    "param_grid_rf = {'criterion' : criterion_range,\n",
    "                 'min_samples_split' : split_range,\n",
    "                 'max_depth' : depth_range,\n",
    "                 'min_samples_leaf' : leaf_range\n",
    "                 }\n",
    "\n",
    "# declaring a hyperparameter space\n",
    "# hyperparameter grid for GBM \n",
    "\n",
    "criterion = ['friedman_mse', 'squared_error', 'mse' ]\n",
    "split  = [2, 5, 10, 15]\n",
    "depth     = range(2, 6 ,1)  \n",
    "leaf      = range(2, 60 ,2)   \n",
    "\n",
    "\n",
    "\n",
    "param_grid_gbm   = {'criterion' : criterion,\n",
    "                    'min_samples_split' : split,\n",
    "                    'max_depth' : depth,\n",
    "                    'min_samples_leaf' : leaf }\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984ec44b",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<h3>Step 7: Hyperparameter Tuning Part III</h3><br>\n",
    "\n",
    "Approximately, the processing time varies from 5 min to 30 min when the number of combinations gets higher (1000 and above).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1faebbc4",
   "metadata": {},
   "source": [
    "**GradientBoostingRegressor Fine Tunning Process** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd9669",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "model_gbm = GradientBoostingRegressor(random_state = 219)\n",
    "\n",
    "\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "model_cv_gbm = RandomizedSearchCV(estimator           = model_gbm,\n",
    "                              param_distributions = param_grid_gbm,\n",
    "                              n_iter              = 1000,\n",
    "                              cv                  = 3,\n",
    "                              random_state        = 219)\n",
    "\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "model_cv.fit(x_data, y_data)\n",
    "\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(f\"Tuned Parameters: {model_cv_gbm.best_params_}\")\n",
    "print(f\"Tuned R-Square:   {model_cv_gbm.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ec6bf",
   "metadata": {},
   "source": [
    "**RandomForestRegressor Fine Tunning Process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e3aa9f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:372: FitFailedWarning: \n",
      "25 fits failed out of a total of 100.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 680, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 450, in fit\n",
      "    trees = Parallel(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 1043, in __call__\n",
      "    if self.dispatch_one_batch(iterator):\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 861, in dispatch_one_batch\n",
      "    self._dispatch(tasks)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 779, in _dispatch\n",
      "    job = self._backend.apply_async(batch, callback=cb)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 208, in apply_async\n",
      "    result = ImmediateResult(func)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 572, in __init__\n",
      "    self.results = batch()\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n",
      "    return [func(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 216, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 185, in _parallel_build_trees\n",
      "    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 1315, in fit\n",
      "    super().fit(\n",
      "  File \"C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\sklearn\\tree\\_classes.py\", line 250, in fit\n",
      "    raise ValueError(\n",
      "ValueError: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\LENOVO\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:969: UserWarning: One or more of the test scores are non-finite: [0.82631931        nan 0.85882986 0.84592274 0.83687729 0.82631931\n",
      " 0.86011454        nan        nan 0.82428606 0.83070884 0.85363379\n",
      " 0.83687729 0.82854561        nan 0.86314174        nan 0.85399633\n",
      " 0.82854561 0.82428606]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned Parameters: {'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 40, 'criterion': 'squared_error'}\n",
      "Tuned R-Square:   0.8631417429230112\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# INSTANTIATING the model object without hyperparameters\n",
    "model_rf = RandomForestRegressor(random_state = 219)\n",
    "\n",
    "\n",
    "\n",
    "# RandomizedSearchCV object\n",
    "model_cv_rf = RandomizedSearchCV(estimator           = model_rf,\n",
    "                              param_distributions = param_grid_rf,\n",
    "                              n_iter              = 20,\n",
    "                              cv                  = 5,\n",
    "                              random_state        = 219)\n",
    "\n",
    "\n",
    "\n",
    "# FITTING to the FULL DATASET (due to cross-validation)\n",
    "model_cv_rf.fit(x_data, y_data)\n",
    "\n",
    "\n",
    "\n",
    "# printing the optimal parameters and best score\n",
    "print(f\"Tuned Parameters: {model_cv_rf.best_params_}\")\n",
    "print(f\"Tuned R-Square:   {model_cv_rf.best_score_}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3102d91",
   "metadata": {},
   "source": [
    "- Exploring the hyperparameter tuning results for one of our best tuned models (for RF and GBM). \n",
    "\n",
    "- Looking for hyperparameter combinations that tied for first place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeec0990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score : 0.9567\n",
      "Testing Score  : 0.8676\n",
      "Train-Test Gap : 0.0891\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a model with tuned values\n",
    "model = RandomForestRegressor(min_samples_split = 5, \n",
    "                              min_samples_leaf = 3, \n",
    "                              max_depth = 30, \n",
    "                              criterion = \"squared_error\", \n",
    "                              random_state     = 219)\n",
    "\n",
    "\n",
    "# FITTING to the TRAINING data\n",
    "model_fit = model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "model_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "model_train_score = model.score(X_train, y_train).round(4) # using R-square\n",
    "model_test_score  = model.score(X_test, y_test).round(4)   # using R-square\n",
    "model_gap         = abs(model_train_score - model_test_score).round(4)\n",
    "\n",
    "\n",
    "# displaying results\n",
    "print('Training Score :', model_train_score)\n",
    "print('Testing Score  :', model_test_score)\n",
    "print('Train-Test Gap :', model_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69d7e5",
   "metadata": {},
   "source": [
    "- Test the best 3 tuned RF models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f2eda585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Model     criterion  max_depth  min_samples_split  min_samples_leaf  Train R-Square  Test R-Square\n",
      "Random Forest (Top 1) squared_error         60                  3                 2           0.970          0.869\n",
      "Random Forest (Top 2) squared_error         30                  5                 3           0.957          0.868\n",
      "Random Forest (Top 3) squared_error         40                 10                 1           0.961          0.869\n"
     ]
    }
   ],
   "source": [
    "# Define the BEST parameters for Random Forest Top1\n",
    "rf_default_params_1 = {'criterion': 'squared_error', 'max_depth': 60, 'min_samples_leaf': 2, 'random_state' :219, 'min_samples_split' : 3}\n",
    "\n",
    "                            \n",
    "rf_default_1 = RandomForestRegressor(**rf_default_params_1)\n",
    "rf_default_1.fit(X_train, y_train)\n",
    "rf_default_train_score_1 = r2_score(y_train, rf_default_1.predict(X_train)).round(3)\n",
    "rf_default_test_score_1 = r2_score(y_test, rf_default_1.predict(X_test)).round(3)\n",
    "\n",
    "# Define the BEST parameters for Random Forest Top2\n",
    "rf_default_params_2 = {'criterion': 'squared_error', 'max_depth': 30, \n",
    "                       'min_samples_leaf': 3, 'random_state' :219, 'min_samples_split' : 5}\n",
    "\n",
    "rf_default_2 = RandomForestRegressor(**rf_default_params_2)\n",
    "rf_default_2.fit(X_train, y_train)\n",
    "rf_default_train_score_2 = r2_score(y_train, rf_default_2.predict(X_train)).round(3)\n",
    "rf_default_test_score_2 = r2_score(y_test, rf_default_2.predict(X_test)).round(3)\n",
    "\n",
    "\n",
    "# Define the BEST parameters for Random Forest Top3\n",
    "rf_default_params_3 = {'random_state' :219,\n",
    "                      'min_samples_split': 10, 'min_samples_leaf': 1, 'max_depth': 40, 'criterion': 'squared_error'}\n",
    "\n",
    "rf_default_3 = RandomForestRegressor(**rf_default_params_3)\n",
    "rf_default_3.fit(X_train, y_train)\n",
    "rf_default_train_score_3 = r2_score(y_train, rf_default_3.predict(X_train)).round(3)\n",
    "rf_default_test_score_3 = r2_score(y_test, rf_default_3.predict(X_test)).round(3)\n",
    "\n",
    "# Define a dictionary to store the results\n",
    "results_dict = {\n",
    "    'Model': ['Random Forest (Top 1)', 'Random Forest (Top 2)', 'Random Forest (Top 3)'],\n",
    "    'criterion': [rf_default_params_1['criterion'], rf_default_params_2['criterion'], rf_default_params_3['criterion']],\n",
    "    'max_depth': [rf_default_params_1['max_depth'], rf_default_params_2['max_depth'], rf_default_params_3['max_depth']],\n",
    "    'min_samples_split': [rf_default_params_1['min_samples_split'], rf_default_params_2['min_samples_split'], rf_default_params_3['min_samples_split']],\n",
    "    'min_samples_leaf': [rf_default_params_1['min_samples_leaf'], rf_default_params_2['min_samples_leaf'], rf_default_params_3['min_samples_leaf']],\n",
    "    'Train R-Square': [rf_default_train_score_1, rf_default_train_score_2, rf_default_train_score_3],\n",
    "    'Test R-Square': [rf_default_test_score_1, rf_default_test_score_2, rf_default_test_score_3]\n",
    "}\n",
    "\n",
    "# Create a Pandas DataFrame from the results dictionary\n",
    "results_df = pd.DataFrame(results_dict)\n",
    "\n",
    "# Print the results in tabular format\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3917a8d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score : 0.9511\n",
      "Testing Score  : 0.8789\n",
      "Train-Test Gap : 0.0722\n"
     ]
    }
   ],
   "source": [
    "# building a model based on hyperparameter tuning results\n",
    "\n",
    "# INSTANTIATING a model with tuned values\n",
    "model = GradientBoostingRegressor(min_samples_split = 2, \n",
    "                              min_samples_leaf = 2, \n",
    "                              max_depth = 4, \n",
    "                              criterion = \"friedman_mse\", \n",
    "                              random_state     = 219)\n",
    "\n",
    "\n",
    "# FITTING to the TRAINING data\n",
    "model_fit = model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# PREDICTING based on the testing set\n",
    "model_pred = model.predict(X_test)\n",
    "\n",
    "\n",
    "# SCORING the results\n",
    "model_train_score = model.score(X_train, y_train).round(4) # using R-square\n",
    "model_test_score  = model.score(X_test, y_test).round(4)   # using R-square\n",
    "model_gap         = abs(model_train_score - model_test_score).round(4)\n",
    "\n",
    "\n",
    "# displaying results\n",
    "print('Training Score :', model_train_score)\n",
    "print('Testing Score  :', model_test_score)\n",
    "print('Train-Test Gap :', model_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f790ec3",
   "metadata": {},
   "source": [
    "- Display the top three models for RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffebbe44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that displays the best hypertuning results and their models' test Rsquare\n",
    "# Source : Pr. Chase Kusterer, Script 5 - Hyperparameter Tuning - Solution, DAT-5390 | Computational Data Analytics with Python\n",
    "def tuning_results(cv_results, n=5):\n",
    "    \"\"\"\n",
    "This function will display the top \"n\" models from hyperparameter tuning,\n",
    "based on \"rank_test_score\".\n",
    "\n",
    "PARAMETERS\n",
    "----------\n",
    "cv_results = results dictionary from the attribute \".cv_results_\"\n",
    "n          = number of models to display\n",
    "    \"\"\"\n",
    "    param_lst = []\n",
    "\n",
    "    for result in cv_results[\"params\"]:\n",
    "        result = str(result).replace(\":\", \"=\")\n",
    "        param_lst.append(result[1:-1])\n",
    "\n",
    "\n",
    "    results_df = pd.DataFrame(data = {\n",
    "        \"Model_Rank\" : cv_results[\"rank_test_score\"],\n",
    "        \"Mean_Test_Score\" : cv_results[\"mean_test_score\"],\n",
    "        \"SD_Test_Score\" : cv_results[\"std_test_score\"],\n",
    "        \"Parameters\" : param_lst\n",
    "    })\n",
    "\n",
    "\n",
    "    results_df = results_df.sort_values(by = \"Model_Rank\", axis = 0)\n",
    "    return results_df.head(n = n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25331622",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model_Rank</th>\n",
       "      <th>Mean_Test_Score</th>\n",
       "      <th>SD_Test_Score</th>\n",
       "      <th>Parameters</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0.863142</td>\n",
       "      <td>0.015025</td>\n",
       "      <td>'min_samples_split'= 10, 'min_samples_leaf'= 1, 'max_depth'= 40, 'criterion'= 'squared_error'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>0.860115</td>\n",
       "      <td>0.017050</td>\n",
       "      <td>'min_samples_split'= 5, 'min_samples_leaf'= 3, 'max_depth'= 20, 'criterion'= 'squared_error'</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.858830</td>\n",
       "      <td>0.017018</td>\n",
       "      <td>'min_samples_split'= 10, 'min_samples_leaf'= 3, 'max_depth'= 85, 'criterion'= 'squared_error'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model_Rank  Mean_Test_Score  SD_Test_Score                                                                                     Parameters\n",
       "15           1         0.863142       0.015025  'min_samples_split'= 10, 'min_samples_leaf'= 1, 'max_depth'= 40, 'criterion'= 'squared_error'\n",
       "6            2         0.860115       0.017050   'min_samples_split'= 5, 'min_samples_leaf'= 3, 'max_depth'= 20, 'criterion'= 'squared_error'\n",
       "2            3         0.858830       0.017018  'min_samples_split'= 10, 'min_samples_leaf'= 3, 'max_depth'= 85, 'criterion'= 'squared_error'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# run tuning_results() on the hyperparameter tuning results, displaying the top 3 best models for RF \n",
    "tuning_results (n=3, cv_results = model_cv_rf.cv_results_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96d5523",
   "metadata": {},
   "source": [
    "- The best parameters for **Random Forest Regressor** (Train score is 0.96 and Test score is 0.869) are : \n",
    "\n",
    "      {'criterion' : 'squared_error', 'max_depth' : 40, 'min_samples_leaf' : 1, 'min_samples_split' : 10}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a6f0a9",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<h3>Step 8: (optional) Explore Tuned Hyperparameters</h3><br>\n",
    "After tuning, it is a great idea to explore any hyperparameters that were markedly different from their default values once tuned. Investigate the purpose of these hyperparameters in <a href=\"https://scikit-learn.org/stable/index.html\">the scikit-learn documentation</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9079d61d",
   "metadata": {},
   "source": [
    "<hr style=\"height:.9px;border:none;color:#333;background-color:#333;\" />\n",
    "<h3> References:</h3><br>\n",
    "\n",
    "- Pr. Chase Kusterer, Script 5 - Hyperparameter Tuning - Solution, DAT-5390 | Computational Data Analytics with Python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
